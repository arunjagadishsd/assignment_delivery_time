{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uOZe3UO68U-"
   },
   "source": [
    "# Order Delivery Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd9e2-HF6_85"
   },
   "source": [
    "## Objectives\n",
    "The objective of this assignment is to build a regression model that predicts the delivery time for orders placed through Porter. The model will use various features such as the items ordered, the restaurant location, the order protocol, and the availability of delivery partners.\n",
    "\n",
    "The key goals are:\n",
    "- Predict the delivery time for an order based on multiple input features\n",
    "- Improve delivery time predictions to optimiae operational efficiency\n",
    "- Understand the key factors influencing delivery time to enhance the model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcC6tJ2p7F2p"
   },
   "source": [
    "## Data Pipeline\n",
    "The data pipeline for this assignment will involve the following steps:\n",
    "1. **Data Loading**\n",
    "2. **Data Preprocessing and Feature Engineering**\n",
    "3. **Exploratory Data Analysis**\n",
    "4. **Model Building**\n",
    "5. **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGOQI_f72jV1"
   },
   "source": [
    "## Data Understanding\n",
    "The dataset contains information on orders placed through Porter, with the following columns:\n",
    "\n",
    "| Field                     | Description                                                                                 |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------|\n",
    "| market_id                 | Integer ID representing the market where the restaurant is located.                         |\n",
    "| created_at                | Timestamp when the order was placed.                                                        |\n",
    "| actual_delivery_time      | Timestamp when the order was delivered.                                                     |\n",
    "| store_primary_category    | Category of the restaurant (e.g., fast food, dine-in).                                      |\n",
    "| order_protocol            | Integer representing how the order was placed (e.g., via Porter, call to restaurant, etc.). |\n",
    "| total_items               | Total number of items in the order.                                                         |\n",
    "| subtotal                  | Final price of the order.                                                                   |\n",
    "| num_distinct_items        | Number of distinct items in the order.                                                      |\n",
    "| min_item_price            | Price of the cheapest item in the order.                                                    |\n",
    "| max_item_price            | Price of the most expensive item in the order.                                              |\n",
    "| total_onshift_dashers     | Number of delivery partners on duty when the order was placed.                              |\n",
    "| total_busy_dashers        | Number of delivery partners already occupied with other orders.                             |\n",
    "| total_outstanding_orders  | Number of orders pending fulfillment at the time of the order.                              |\n",
    "| distance                  | Total distance from the restaurant to the customer.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QoCQFDzHUWP"
   },
   "source": [
    "## **Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jun9CeAc7QOw"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MueJxkvUIII3"
   },
   "source": [
    "## **1. Loading the data**\n",
    "Load 'porter_data_1.csv' as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJS8ZRJXHTwv"
   },
   "outputs": [],
   "source": [
    "# Importing the file porter_data_1.csv\n",
    "df = pd.read_csv('porter_data_1.csv')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSRQocOkMSQl"
   },
   "source": [
    "## **2. Data Preprocessing and Feature Engineering** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02uPO8aQfLnn"
   },
   "source": [
    "#### **2.1 Fixing the Datatypes**  <font color = red>[5 marks]</font> <br>\n",
    "The current timestamps are in object format and need conversion to datetime format for easier handling and intended functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b22Kzjew3rdM"
   },
   "source": [
    "##### **2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "Convert date and time fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoGkz909IXjv"
   },
   "outputs": [],
   "source": [
    "# Convert 'created_at' and 'actual_delivery_time' columns to datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EBPjFc4Qca"
   },
   "source": [
    "##### **2.1.2**  <font color = red>[3 marks]</font> <br>\n",
    "Convert categorical fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihPSPhQq1nQ"
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to category type\n",
    "categorical_columns = ['store_primary_category', 'order_protocol', 'market_id']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsEGroRFlX8z"
   },
   "source": [
    "#### **2.2 Feature Engineering** <font color = red>[5 marks]</font> <br>\n",
    "Calculate the time taken to execute the delivery as well as extract the hour and day at which the order was placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BubGzQyJpHLQ"
   },
   "source": [
    "##### **2.2.1** <font color = red>[2 marks]</font> <br>\n",
    "Calculate the time taken using the features `actual_delivery_time` and `created_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGS4PZJMciZ"
   },
   "outputs": [],
   "source": [
    "# Calculate time taken in minutes\n",
    "df['time_taken'] = (df['actual_delivery_time'] - df['created_at']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngUUAf3XOPAP"
   },
   "source": [
    "##### **2.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Extract the hour at which the order was placed and which day of the week it was. Drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwA4O5VtNxQW"
   },
   "outputs": [],
   "source": [
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Create a categorical feature 'isWeekend'\n",
    "df['isWeekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgzSO8wyOTbP"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(columns=['created_at', 'actual_delivery_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJxTsQOFKyl"
   },
   "source": [
    "#### **2.3 Creating training and validation sets** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuyPJMpCFyUL"
   },
   "source": [
    "##### **2.3.1** <font color = red>[2 marks]</font> <br>\n",
    " Define target and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVyKFLXTFKRE"
   },
   "outputs": [],
   "source": [
    "# Define target variable (y) and features (X)\n",
    "y = df['time_taken']\n",
    "\n",
    "# Define features (X) - excluding the target variable\n",
    "X = df.drop(['time_taken'], axis=1)\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "X = pd.get_dummies(X, columns=['store_primary_category', 'order_protocol', 'market_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56iVNqdF3G8"
   },
   "source": [
    "##### **2.3.2** <font color = red>[3 marks]</font> <br>\n",
    " Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t7XtNDEF6Pu"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQxv96NBAq_y"
   },
   "source": [
    "## **3. Exploratory Data Analysis on Training Data** <font color = red>[20 marks]</font> <br>\n",
    "1. Analyzing the correlation between variables to identify patterns and relationships\n",
    "2. Identifying and addressing outliers to ensure the integrity of the analysis\n",
    "3. Exploring the relationships between variables and examining the distribution of the data for better insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1baEcRc1-A"
   },
   "source": [
    "#### **3.1 Feature Distributions** <font color = red> [7 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj7yFI7VJ_va"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define numerical columns\n",
    "numerical_columns = ['total_items', 'subtotal', 'num_distinct_items', \n",
    "                    'min_item_price', 'max_item_price', 'total_onshift_dashers',\n",
    "                    'total_busy_dashers', 'total_outstanding_orders', \n",
    "                    'distance', 'hour', 'time_taken']\n",
    "\n",
    "# Define categorical columns \n",
    "categorical_columns = ['store_primary_category', 'order_protocol', \n",
    "                     'market_id', 'day_of_week', 'isWeekend']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWMFLWKpHE-R"
   },
   "source": [
    "##### **3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot distributions for numerical columns in the training set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M0u5G1YR73_"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.suptitle('Distribution of Numerical Features', fontsize=16)\n",
    "\n",
    "# Plot histograms for each numerical column\n",
    "for i, col in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(data=X_train[col] if col != 'time_taken' else y_train, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MtpapIvc9rC"
   },
   "source": [
    "##### **3.1.2** <font color = red>[2 marks]</font> <br>\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr8loNgMLdrm"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribution of Categorical Features', fontsize=16)\n",
    "\n",
    "# Plot bar charts for each categorical column\n",
    "for i, col in enumerate(categorical_columns, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    value_counts = X_train[col].value_counts() if col in X_train.columns else X_train.filter(like=col).sum()\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-9pcLxzJZWf"
   },
   "source": [
    "##### **3.1.3** <font color = red>[2 mark]</font> <br>\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiWe2Bl9R7yL"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Distribution of Delivery Time', fontsize=16)\n",
    "\n",
    "# Plot histogram with KDE\n",
    "sns.histplot(data=y_train, kde=True, bins=50)\n",
    "plt.xlabel('Time Taken (minutes)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add vertical lines for key statistics\n",
    "plt.axvline(y_train.mean(), color='red', linestyle='--', label=f'Mean: {y_train.mean():.2f}')\n",
    "plt.axvline(y_train.median(), color='green', linestyle='--', label=f'Median: {y_train.median():.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbxczs61dROZ"
   },
   "source": [
    "#### **3.2 Relationships Between Features** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH81kNkOOvlx"
   },
   "source": [
    "##### **3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Scatter plots for important numerical and categorical features to observe how they relate to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIBnRHohR799"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.suptitle('Relationships between Delivery Time and Key Features', fontsize=16)\n",
    "\n",
    "# Select important numerical features to plot\n",
    "important_features = ['distance', 'total_items', 'subtotal', 'total_outstanding_orders']\n",
    "\n",
    "# Create scatter plots\n",
    "for i, feature in enumerate(important_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Create scatter plot with regression line\n",
    "    sns.regplot(data=X_train, \n",
    "                x=feature,\n",
    "                y=y_train,\n",
    "                scatter_kws={'alpha':0.5},\n",
    "                line_kws={'color': 'red'})\n",
    "    \n",
    "    plt.title(f'Delivery Time vs {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Time Taken (minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values\n",
    "print(\"\\nCorrelations with delivery time:\")\n",
    "for feature in important_features:\n",
    "    correlation = X_train[feature].corr(y_train)\n",
    "    print(f\"{feature}: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiWL3cKowfZd"
   },
   "outputs": [],
   "source": [
    "# Show the distribution of time_taken for different hours\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.suptitle('Delivery Time Distribution by Hour of Day', fontsize=16)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(data=X_train, x='hour', y=y_train)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Delivery Time (minutes)')\n",
    "plt.xticks(range(24))\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add mean delivery time line\n",
    "mean_time = y_train.mean()\n",
    "plt.axhline(y=mean_time, color='red', linestyle='--', \n",
    "            label=f'Overall Mean: {mean_time:.1f} min')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics by hour\n",
    "hourly_stats = pd.DataFrame({\n",
    "    'Mean Time': y_train.groupby(X_train['hour']).mean(),\n",
    "    'Median Time': y_train.groupby(X_train['hour']).median(),\n",
    "    'Count': y_train.groupby(X_train['hour']).count()\n",
    "}).round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKg6rBljIJFP"
   },
   "source": [
    "#### **3.3 Correlation Analysis** <font color = red>[5 marks]</font> <br>\n",
    "Check correlations between numerical features to identify which variables are strongly related to `time_taken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyk00sbYfnc0"
   },
   "source": [
    "##### **3.3.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot a heatmap to display correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxrdHdvKR7vy"
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap of the correlation matrix\n",
    "correlation_matrix = pd.DataFrame(X_train[numerical_columns[:-1]]) # Exclude time_taken\n",
    "correlation_matrix['time_taken'] = y_train\n",
    "corr = correlation_matrix.corr()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.suptitle('Correlation Matrix Heatmap', fontsize=16)\n",
    "\n",
    "# Create heatmap with annotations\n",
    "sns.heatmap(corr, \n",
    "            annot=True,          # Show correlation values\n",
    "            cmap='coolwarm',     # Color scheme\n",
    "            center=0,            # Center the colormap at 0\n",
    "            square=True,         # Make cells square\n",
    "            fmt='.2f',           # Format annotations to 2 decimal places\n",
    "            linewidths=0.5,      # Add gridlines\n",
    "            cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations with time_taken\n",
    "print(\"\\nFeatures most correlated with delivery time:\")\n",
    "correlations = corr['time_taken'].sort_values(ascending=False)\n",
    "print(correlations.drop('time_taken'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yuD3RIwffZE"
   },
   "source": [
    "##### **3.3.2** <font color = red>[2 marks]</font> <br>\n",
    "Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDZN586gH8R_"
   },
   "outputs": [],
   "source": [
    "# Drop 3-5 weakly correlated columns from training dataset\n",
    "correlations = abs(correlations)\n",
    "\n",
    "# Find features with correlations below threshold (e.g., 0.1)\n",
    "weak_correlations = correlations[correlations < 0.1].index.tolist()\n",
    "\n",
    "print(\"Features with weak correlations:\")\n",
    "for feature in weak_correlations:\n",
    "    print(f\"{feature}: {correlations[feature]:.3f}\")\n",
    "\n",
    "# Drop weakly correlated columns from training and test sets\n",
    "X_train = X_train.drop(columns=weak_correlations)\n",
    "X_test = X_test.drop(columns=weak_correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mZv2rz6lxvc"
   },
   "source": [
    "#### **3.4 Handling the Outliers** <font color = red>[5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdyAT-OhyH3z"
   },
   "source": [
    "##### **3.4.1** <font color = red>[2 marks]</font> <br>\n",
    "Visualise potential outliers for the target variable and other numerical features using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow3Mowo4R71T"
   },
   "outputs": [],
   "source": [
    "# Boxplot for time_taken\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribution and Outliers in Numerical Features', fontsize=16)\n",
    "\n",
    "# Create boxplots for numerical features including time_taken\n",
    "all_features = X_train[numerical_columns[:-1]].copy()  # Get numerical features\n",
    "all_features['time_taken'] = y_train  # Add target variable\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(data=all_features)\n",
    "\n",
    "# Customize plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary statistics for time_taken:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "# Calculate outlier boundaries for time_taken\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nOutlier boundaries for time_taken:\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper bound: {upper_bound:.2f}\")\n",
    "print(f\"Number of outliers: {len(y_train[(y_train < lower_bound) | (y_train > upper_bound)])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZCaGBKv_stm"
   },
   "source": [
    "##### **3.4.2** <font color = red>[3 marks]</font> <br>\n",
    "Handle outliers present in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwQ1A_wZ_X_K"
   },
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "def remove_outliers(df, columns, n_std=1.5):\n",
    "    \"\"\"Remove outliers from specified columns using IQR method\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    for column in columns:\n",
    "        # Calculate Q1, Q3 and IQR\n",
    "        Q1 = df_clean[column].quantile(0.25)\n",
    "        Q3 = df_clean[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - n_std * IQR\n",
    "        upper_bound = Q3 + n_std * IQR\n",
    "        \n",
    "        # Remove outliers\n",
    "        df_clean = df_clean[\n",
    "            (df_clean[column] >= lower_bound) & \n",
    "            (df_clean[column] <= upper_bound)\n",
    "        ]\n",
    "    return df_clean\n",
    "\n",
    "# Select numerical columns to handle outliers\n",
    "columns_to_clean = ['time_taken', 'distance', 'total_items', \n",
    "                    'subtotal', 'total_outstanding_orders']\n",
    "\n",
    "# Print original shapes\n",
    "print(\"Original shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "# Create DataFrame combining features and target for cleaning\n",
    "train_data = X_train.copy()\n",
    "train_data['time_taken'] = y_train\n",
    "\n",
    "# Remove outliers\n",
    "train_data_clean = remove_outliers(train_data, columns_to_clean)\n",
    "\n",
    "# Split back into features and target\n",
    "X_train_clean = train_data_clean.drop('time_taken', axis=1)\n",
    "y_train_clean = train_data_clean['time_taken']\n",
    "\n",
    "# Print new shapes and reduction percentage\n",
    "print(\"\\nAfter outlier removal:\")\n",
    "print(f\"X_train: {X_train_clean.shape}\")\n",
    "print(f\"y_train: {y_train_clean.shape}\")\n",
    "print(f\"Removed {(len(y_train) - len(y_train_clean))/len(y_train)*100:.1f}% of records\")\n",
    "\n",
    "# Update training data\n",
    "X_train = X_train_clean\n",
    "y_train = y_train_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0Cd2J-LGWaF"
   },
   "source": [
    "## **4. Exploratory Data Analysis on Validation Data** <font color = red>[optional]</font> <br>\n",
    "Optionally, perform EDA on test data to see if the distribution match with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sN6bG_hTbUE"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define numerical columns (excluding time_taken)\n",
    "numerical_columns_test = ['total_items', 'subtotal', 'num_distinct_items', \n",
    "                         'min_item_price', 'max_item_price', 'total_onshift_dashers',\n",
    "                         'total_busy_dashers', 'total_outstanding_orders', \n",
    "                         'distance', 'hour']\n",
    "\n",
    "# Define categorical columns (original categories before dummy encoding)\n",
    "categorical_columns_test = ['store_primary_category', 'order_protocol', \n",
    "                          'market_id', 'day_of_week', 'isWeekend']\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zq16lr0Q9IG"
   },
   "source": [
    "#### **4.1 Feature Distributions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuoIVgXlQC9y"
   },
   "source": [
    "##### **4.1.1**\n",
    "Plot distributions for numerical columns in the validation set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKgSvKvzG8fv"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "\n",
    "# Plot distributions for numerical columns in test set\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.suptitle('Distribution of Numerical Features (Test Set)', fontsize=16)\n",
    "\n",
    "# Plot histograms for each numerical column\n",
    "for i, col in enumerate(numerical_columns_test, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(data=X_test[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrywBQGWQC9z"
   },
   "source": [
    "##### **4.1.2**\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CIcl2tHBwp"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "\n",
    "# Distribution of categorical columns in test set\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribution of Categorical Features (Test Set)', fontsize=16)\n",
    "\n",
    "# Plot bar charts for each categorical column\n",
    "for i, col in enumerate(categorical_columns_test, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    value_counts = X_test[col].value_counts() if col in X_test.columns else X_test.filter(like=col).sum()\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_j74bnlQC9z"
   },
   "source": [
    "##### **4.1.3**\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dGfR8MHGtqm"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "\n",
    "# Distribution of time_taken in test set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Distribution of Delivery Time (Test Set)', fontsize=16)\n",
    "\n",
    "# Plot histogram with KDE\n",
    "sns.histplot(data=y_test, kde=True, bins=50)\n",
    "plt.xlabel('Time Taken (minutes)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add vertical lines for key statistics\n",
    "plt.axvline(y_test.mean(), color='red', linestyle='--', label=f'Mean: {y_test.mean():.2f}')\n",
    "plt.axvline(y_test.median(), color='green', linestyle='--', label=f'Median: {y_test.median():.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare basic statistics between train and test sets\n",
    "print('\\nComparison of time_taken statistics:')\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Training': y_train.describe(),\n",
    "    'Test': y_test.describe()\n",
    "})\n",
    "print(stats_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki2FI7fsHDgK"
   },
   "source": [
    "#### **4.2 Relationships Between Features**\n",
    "Scatter plots for numerical features to observe how they relate to each other, especially to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lzNPoK4HFnZ"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "\n",
    "# Scatter plots for key features in test set\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.suptitle('Relationships between Delivery Time and Key Features (Test Set)', fontsize=16)\n",
    "\n",
    "# Select important numerical features\n",
    "important_features = ['distance', 'total_items', 'subtotal', 'total_outstanding_orders']\n",
    "\n",
    "# Create scatter plots\n",
    "for i, feature in enumerate(important_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.regplot(data=X_test, \n",
    "                x=feature,\n",
    "                y=y_test,\n",
    "                scatter_kws={'alpha':0.5},\n",
    "                line_kws={'color': 'red'})\n",
    "    plt.title(f'Delivery Time vs {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Time Taken (minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values\n",
    "print(\"\\nCorrelations with delivery time (Test Set):\")\n",
    "for feature in important_features:\n",
    "    correlation = X_test[feature].corr(y_test)\n",
    "    print(f\"{feature}: {correlation:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8VoM0XfXWko"
   },
   "source": [
    "#### **4.3** Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BnM8w2lXWkp"
   },
   "outputs": [],
   "source": [
    "# Drop the weakly correlated columns from training dataset\n",
    "\n",
    "# Calculate correlations in test set\n",
    "test_correlation_matrix = pd.DataFrame(X_test[numerical_columns_test])\n",
    "test_correlation_matrix['time_taken'] = y_test\n",
    "test_corr = abs(test_correlation_matrix.corr()['time_taken'])\n",
    "\n",
    "# Find features with weak correlations\n",
    "weak_correlations_test = test_corr[test_corr < 0.1].index.tolist()\n",
    "\n",
    "print(\"Features with weak correlations in test set:\")\n",
    "for feature in weak_correlations_test:\n",
    "    print(f\"{feature}: {test_corr[feature]:.3f}\")\n",
    "\n",
    "# Compare weak correlations between train and test\n",
    "print(\"\\nComparison of weak correlations:\")\n",
    "print(f\"Train set: {weak_correlations}\")\n",
    "print(f\"Test set: {weak_correlations_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNN4PyM8enl"
   },
   "source": [
    "## **5. Model Building** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l2XfNF6nc8L"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__fmfT6vQWpd"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCLIKw5pQiA7"
   },
   "source": [
    "#### **5.1 Feature Scaling** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "newEgSyyQiHK"
   },
   "outputs": [],
   "source": [
    "# Apply scaling to the numerical columns\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Scale numerical features\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print('Scaled features:', numerical_features.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXcV5Z_E8tLL"
   },
   "source": [
    "Note that linear regression is agnostic to feature scaling. However, with feature scaling, we get the coefficients to be somewhat on the same scale so that it becomes easier to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxip-t3Y1MB"
   },
   "source": [
    "#### **5.2 Build a linear regression model** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7jZciTFtric"
   },
   "source": [
    "You can choose from the libraries *statsmodels* and *scikit-learn* to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMRpgx_iQYM4"
   },
   "outputs": [],
   "source": [
    "# Create/Initialise the model\n",
    "\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize RFE with 8 features\n",
    "selector = RFE(estimator=model, n_features_to_select=8, step=1)\n",
    "\n",
    "# Fit RFE\n",
    "selector = selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_train_scaled.columns[selector.support_].tolist()\n",
    "print('Selected features:', selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbJVZpMiW8b2"
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "\n",
    "# Train model with selected features\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_scaled[selected_features], y_train)\n",
    "\n",
    "# Print model summary\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Coefficient': final_model.coef_\n",
    "})\n",
    "print('Model Coefficients:')\n",
    "print(coef_df.sort_values(by='Coefficient', ascending=False))\n",
    "print(f'\\nIntercept: {final_model.intercept_:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCQcJtDbW_dG"
   },
   "outputs": [],
   "source": [
    "# Make predictions on train and test sets\n",
    "y_train_pred = final_model.predict(X_train_scaled[selected_features])\n",
    "y_test_pred = final_model.predict(X_test_scaled[selected_features])\n",
    "\n",
    "# Calculate residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Udw5kE1fXBsR"
   },
   "outputs": [],
   "source": [
    "# Find results for evaluation metrics\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print('Training Metrics:')\n",
    "print(f'RMSE: {train_rmse:.2f}')\n",
    "print(f'R²: {train_r2:.3f}')\n",
    "\n",
    "print('\\nTest Metrics:')\n",
    "print(f'RMSE: {test_rmse:.2f}')\n",
    "print(f'R²: {test_r2:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3-HovybcZKR"
   },
   "source": [
    "Note that we have 12 (depending on how you select features) training features. However, not all of them would be useful. Let's say we want to take the most relevant 8 features.\n",
    "\n",
    "We will use Recursive Feature Elimination (RFE) here.\n",
    "\n",
    "For this, you can look at the coefficients / p-values of features from the model summary and perform feature elimination, or you can use the RFE module provided with *scikit-learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU8OLQ4bnwdr"
   },
   "source": [
    "#### **5.3 Build the model and fit RFE to select the most important features** <font color = red>[7 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FZMiX11RyI"
   },
   "source": [
    "For RFE, we will start with all features and use\n",
    "the RFE method to recursively reduce the number of features one-by-one.\n",
    "\n",
    "After analysing the results of these iterations, we select the one that has a good balance between performance and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub1HgSwl1eiC"
   },
   "outputs": [],
   "source": [
    "# Loop through the number of features and test the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7p-CAQn3wQE"
   },
   "outputs": [],
   "source": [
    "# Build the final model with selected number of features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l_mLL_4OOl"
   },
   "source": [
    "## **6. Results and Inference** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsPGaacJ71mt"
   },
   "source": [
    "#### **6.1 Perform Residual Analysis** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbj7O8rf7SZS"
   },
   "outputs": [],
   "source": [
    "# Perform residual analysis using plots like residuals vs predicted values, Q-Q plot and residual histogram\n",
    "\n",
    "# Create figure for residual plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted Values\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_train_pred, train_residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "\n",
    "# 2. Residual Histogram\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(train_residuals, bins=30, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "plt.subplot(2, 2, 3)\n",
    "stats.probplot(train_residuals, dist='norm', plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "\n",
    "# 4. Residuals vs Order\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_residuals, marker='o', linestyle='none', alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Order')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Order')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print residual statistics\n",
    "print('Residual Statistics:')\n",
    "print(f'Mean: {np.mean(train_residuals):.2f}')\n",
    "print(f'Std Dev: {np.std(train_residuals):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq4g9xPsu4T5"
   },
   "source": [
    "[Your inferences here:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2-CiCId7_y9"
   },
   "source": [
    "#### **6.2 Perform Coefficient Analysis** <font color = red>[2 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2koFJovu-cH"
   },
   "source": [
    "Perform coefficient analysis to find how changes in features affect the target.\n",
    "Also, the features were scaled, so interpret the scaled and unscaled coefficients to understand the impact of feature changes on delivery time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr8EWhg_9QnI"
   },
   "outputs": [],
   "source": [
    "# Compare the scaled vs unscaled features used in the final model\n",
    "\n",
    "# Compare scaled vs unscaled coefficients\n",
    "scaled_coef = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Scaled_Coefficient': final_model.coef_,\n",
    "    'Abs_Impact': abs(final_model.coef_)\n",
    "}).sort_values('Abs_Impact', ascending=False)\n",
    "\n",
    "# Calculate unscaled coefficients\n",
    "unscaled_coef = final_model.coef_ / scaler.scale_[numerical_features.isin(selected_features)]\n",
    "unscaled_coef = pd.DataFrame({\n",
    "    'Feature': [f for f in selected_features if f in numerical_features],\n",
    "    'Unscaled_Coefficient': unscaled_coef\n",
    "})\n",
    "\n",
    "print('Feature Importance (Scaled Coefficients):')\n",
    "print(scaled_coef)\n",
    "\n",
    "print('\\nUnscaled Coefficients (Original Units):')\n",
    "print(unscaled_coef)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ5VcQ2G-SOb"
   },
   "source": [
    "Additionally, we can analyse the effect of a unit change in a feature. In other words, because we have scaled the features, a unit change in the features will not translate directly to the model. Use scaled and unscaled coefficients to find how will a unit change in a feature affect the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMHN7r-x-Lp5"
   },
   "outputs": [],
   "source": [
    "# Analyze the effect of a unit change in a feature, say 'total_items'\n",
    "\n",
    "# Analyze effect of unit change in important features\n",
    "for feature in ['total_items', 'distance', 'total_outstanding_orders']:\n",
    "    if feature in numerical_features:\n",
    "        # Get the scaled coefficient\n",
    "        coef = final_model.coef_[selected_features.index(feature)]\n",
    "        # Get the scaling parameters\n",
    "        scale = scaler.scale_[numerical_features.get_loc(feature)]\n",
    "        mean = scaler.mean_[numerical_features.get_loc(feature)]\n",
    "        \n",
    "        # Calculate impact of one unit change\n",
    "        unit_impact = coef / scale\n",
    "        \n",
    "        print(f'\\nFeature: {feature}')\n",
    "        print(f'One unit increase leads to {unit_impact:.2f} minute change in delivery time')\n",
    "        print(f'Mean value: {mean:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFWJ2s9I_Yeo"
   },
   "source": [
    "Note:\n",
    "The coefficients on the original scale might differ greatly in magnitude from the scaled coefficients, but they both describe the same relationships between variables.\n",
    "\n",
    "Interpretation is key: Focus on the direction and magnitude of the coefficients on the original scale to understand the impact of each variable on the response variable in the original units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClCit1tvKIyE"
   },
   "source": [
    "Include conclusions in your report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-wDgoeSiHP"
   },
   "source": [
    "## Subjective Questions <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions only in the notebook. Include the visualisations/methodologies/insights/outcomes from all the above steps in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVJSi-Q0Cw_r"
   },
   "source": [
    "#### Subjective Questions based on Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jiT95xTA6q"
   },
   "source": [
    "##### **Question 1.** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Are there any categorical variables in the data? From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvFQvBy3VM9A"
   },
   "source": [
    "**Answer:**\n",
    "> Yes, there are several categorical variables in the data:\n",
    "> 1. store_primary_category (restaurant type)\n",
    "> 2. order_protocol (order placement method)\n",
    "> 3. market_id (location identifier)\n",
    "> 4. day_of_week and isWeekend (derived temporal features)\n",
    ">\n",
    "> From one-hot encoding and subsequent analysis, we can see these variables contribute to delivery time variations. Different restaurant categories and protocols show distinct delivery time patterns, while market locations impact transit times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPxxtWEY3_W"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDSRymTJTHCW"
   },
   "source": [
    "##### **Question 2.** <font color = red>[1 marks]</font> <br>\n",
    "What does `test_size = 0.2` refer to during splitting the data into training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRBCcZvoVx-r"
   },
   "source": [
    "**Answer:**\n",
    "> test_size = 0.2 means that 20% of the dataset is reserved for testing while 80% is used for training. In other words, if we have 1000 samples, 800 will be used for training the model and 200 for testing its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_afbTV8Y5-F"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEVX57VbTJBP"
   },
   "source": [
    "##### **Question 3.** <font color = red>[1 marks]</font> <br>\n",
    "Looking at the heatmap, which one has the highest correlation with the target variable?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewPqz4yLWBzR"
   },
   "source": [
    "**Answer:**\n",
    "> Looking at the correlation heatmap, 'distance' has the highest correlation with the target variable 'time_taken'. This makes intuitive sense as longer distances typically require more delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLy_-8F5Y69c"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-6E-N-TKyS"
   },
   "source": [
    "##### **Question 4.** <font color = red>[2 marks]</font> <br>\n",
    "What was your approach to detect the outliers? How did you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUDtsRGWLZl"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">\n",
    "We used the Interquartile Range (IQR) method to detect outliers:\n",
    "1. Calculated Q1 (25th percentile) and Q3 (75th percentile)\n",
    "2. Computed IQR = Q3 - Q1\n",
    "3. Defined outliers as values outside [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "We addressed outliers by removing records that fell outside these boundaries for key numerical features (time_taken, distance, total_items, subtotal, total_outstanding_orders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVyJFcT2Y7U8"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvh9CLFnTMhO"
   },
   "source": [
    "##### **Question 5.** <font color = red>[2 marks]</font> <br>\n",
    "Based on the final model, which are the top 3 features significantly affecting the delivery time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-DDpZcCWUun"
   },
   "source": [
    "**Answer:**\n",
    "> The top 3 features significantly affecting delivery time are:\n",
    "> 1. Distance (strongest positive correlation)\n",
    "> 2. Total outstanding orders (indicates system load)\n",
    "> 3. Total items (order size)\n",
    "> These features show the strongest correlations and highest coefficient magnitudes in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVCrLjhTY74h"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBLH_lA5C4jy"
   },
   "source": [
    "#### General Subjective Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MJGDVyiTOyr"
   },
   "source": [
    "##### **Question 6.** <font color = red>[3 marks]</font> <br>\n",
    "Explain the linear regression algorithm in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZc1QX8RW_Pa"
   },
   "source": [
    "**Answer:**\n",
    "> Linear regression is a supervised learning algorithm that models the relationship between a dependent variable (y) and one or more independent variables (x) by fitting a linear equation:\n",
    "> y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
    ">\n",
    "> The algorithm:\n",
    "> 1. Takes input features and target variable\n",
    "> 2. Assumes a linear relationship\n",
    "> 3. Uses Ordinary Least Squares (OLS) to minimize the sum of squared residuals\n",
    "> 4. Finds optimal coefficients (β) that best fit the data\n",
    "> 5. Creates a model that can predict target values for new inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0MCb30NY8UE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_7gqf8TQTk"
   },
   "source": [
    "##### **Question 7.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between simple linear regression and multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jsR8htXD8j"
   },
   "source": [
    "**Answer:**\n",
    "> Simple Linear Regression:\n",
    "> - Uses only one independent variable (x)\n",
    "> - Models relationship as y = β₀ + β₁x + ε\n",
    "> - Visualized as a straight line in 2D\n",
    ">\n",
    "> Multiple Linear Regression:\n",
    "> - Uses two or more independent variables\n",
    "> - Models as y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
    "> - Represents a hyperplane in multi-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSGZEltY8ss"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT6ivEEnTSEs"
   },
   "source": [
    "##### **Question 8.** <font color = red>[2 marks]</font> <br>\n",
    "What is the role of the cost function in linear regression, and how is it minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2PaCL-FXSSn"
   },
   "source": [
    "**Answer:**\n",
    "> The cost function in linear regression, typically Mean Squared Error (MSE), measures how well the model fits the data:\n",
    "> MSE = (1/n)Σ(yᵢ - ŷᵢ)²\n",
    ">\n",
    "> It's minimized through:\n",
    "> 1. Gradient Descent: Iteratively adjusts coefficients\n",
    "> 2. Ordinary Least Squares: Analytical solution for minimum\n",
    "> 3. The goal is to find coefficients that produce the smallest possible MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIKB_W0FY9QM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIb5hbMCCVY"
   },
   "source": [
    "##### **Question 9.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between overfitting and underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8kn4c-7CEjP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">\n",
    "> Overfitting:\n",
    "> - Model learns noise in training data\n",
    "> - High training accuracy but poor generalization\n",
    "> - Too complex, captures random fluctuations\n",
    ">\n",
    "> Underfitting:\n",
    "> - Model fails to capture underlying patterns\n",
    "> - Poor performance on both training and test data\n",
    "> - Too simple to represent the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PWIs-suCMEr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7JPKHwArn7"
   },
   "source": [
    "##### **Question 10.** <font color = red>[3 marks]</font> <br>\n",
    "How do residual plots help in diagnosing a linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxU8GSkAubl"
   },
   "source": [
    "**Answer:**\n",
    "> Residual plots help diagnose linear regression models by revealing:\n",
    "> 1. Linearity: Pattern in residuals suggests non-linear relationships\n",
    "> 2. Homoscedasticity: Consistent variance across predictions\n",
    "> 3. Normality: Through Q-Q plots of residuals\n",
    "> 4. Outliers: Points with large residuals\n",
    "> 5. Model adequacy: Random scatter indicates good fit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MueJxkvUIII3",
    "02uPO8aQfLnn",
    "b22Kzjew3rdM",
    "u1EBPjFc4Qca",
    "-JJxTsQOFKyl",
    "v0Cd2J-LGWaF",
    "fCLIKw5pQiA7",
    "2bxip-t3Y1MB",
    "mn-wDgoeSiHP"
   ],
   "provenance": [
    {
     "file_id": "1qHefVpjLoVZcdohzmYySNZGiPR4wQOFp",
     "timestamp": 1737728120597
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
